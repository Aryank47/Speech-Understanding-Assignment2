{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For evaluation metrics (EER, TAR) we use scikit‑learn\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# For SDR, SIR, SAR evaluation (using mir_eval; see https://github.com/craffel/mir_eval)\n",
    "import mir_eval\n",
    "\n",
    "# For PESQ (see https://pypi.org/project/pesq/)\n",
    "from pesq import pesq\n",
    "import gc\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. ArcFace Loss Implementation \n",
    "#    (Reference: Deng et al., “ArcFace: Additive Angular Margin Loss for Deep Face Recognition”, https://arxiv.org/abs/1801.07698)\n",
    "# =============================================================================\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, margin=0.5, scale=64):\n",
    "        \"\"\"\n",
    "        embedding_size: dimension of speaker embeddings\n",
    "        num_classes: number of speaker classes\n",
    "        margin: angular margin penalty\n",
    "        scale: scaling factor for logits\n",
    "        \"\"\"\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def forward(self, embeddings, labels):\n",
    "        # Normalize embeddings and weights\n",
    "        normed_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        normed_weight = F.normalize(self.weight, p=2, dim=1)\n",
    "        cosine = F.linear(normed_embeddings, normed_weight)  # [batch, num_classes]\n",
    "        # Add angular margin\n",
    "        theta = torch.acos(torch.clamp(cosine, -1+1e-7, 1-1e-7))\n",
    "        target_logits = torch.cos(theta + self.margin)\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
    "        output = cosine * (1 - one_hot) + target_logits * one_hot\n",
    "        output = output * self.scale\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. LoRA (Low-Rank Adaptation) Implementation\n",
    "#    (Reference: Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models”, https://arxiv.org/abs/2106.09685)\n",
    "# =============================================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear, r=4, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Wraps an existing nn.Linear module with a low‑rank update.\n",
    "        r: rank of the low‑rank adaptation\n",
    "        alpha: scaling factor\n",
    "        \"\"\"\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.original_linear = original_linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.lora_A = nn.Parameter(torch.randn(original_linear.out_features, r) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.randn(r, original_linear.in_features) * 0.01)\n",
    "        # Freeze original weights\n",
    "        for param in self.original_linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Standard linear output plus low‑rank update\n",
    "        lora_A = self.lora_A.to(x.device)\n",
    "        lora_B = self.lora_B.to(x.device)\n",
    "        return self.original_linear(x) + self.alpha * (x @ lora_B.t() @ lora_A.t())\n",
    "        # return self.original_linear(x) + self.alpha * (x @ self.lora_B.t() @ self.lora_A.t())\n",
    "\n",
    "def get_parent_module(model, module_name):\n",
    "    names = module_name.split('.')\n",
    "    parent = model\n",
    "    for n in names[:-1]:\n",
    "        parent = getattr(parent, n)\n",
    "    return parent\n",
    "\n",
    "def apply_lora(model, r=4, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Iterates over all submodules and replaces nn.Linear modules with LoRALinear.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            parent = get_parent_module(model, name)\n",
    "            child_name = name.split('.')[-1]\n",
    "            setattr(parent, child_name, LoRALinear(module, r, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_audio(waveform, sr, max_duration):\n",
    "    \"\"\"\n",
    "    Trims the waveform to max_duration seconds.\n",
    "    \"\"\"\n",
    "    max_samples = int(max_duration * sr)\n",
    "    if waveform.size(1) > max_samples:\n",
    "        return waveform[:, :max_samples]\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Dataset Definitions\n",
    "# =============================================================================\n",
    "class VoxCelebDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VoxCeleb2 data for fine‑tuning.\n",
    "    Assumes directory structure like: vox2/aac/<identity>/.../<file>.m4a\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, identities, transform=None, file_ext='.m4a'):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        for identity in identities:\n",
    "            id_path = os.path.join(root_dir, identity)\n",
    "            files = glob.glob(os.path.join(id_path, '**', f'*{file_ext}'), recursive=True)\n",
    "            for file in files:\n",
    "                self.samples.append((identity, file))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        identity, file_path = self.samples[idx]\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        return identity, waveform, sr\n",
    "\n",
    "class VoxCelebTrialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for VoxCeleb1 trial pairs.\n",
    "    Expects a text file with lines formatted as:\n",
    "      <label> <enrollment_path> <test_path>\n",
    "    where paths are relative to a given root (e.g. vox1/wav).\n",
    "    \"\"\"\n",
    "    def __init__(self, trial_file, root_dir):\n",
    "        self.trials = []\n",
    "        self.root_dir = root_dir\n",
    "        with open(trial_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                label = int(parts[0])\n",
    "                enrollment = os.path.join(root_dir, parts[1])\n",
    "                test = os.path.join(root_dir, parts[2])\n",
    "                self.trials.append((label, enrollment, test))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trials)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, enroll_path, test_path = self.trials[idx]\n",
    "        enroll_waveform, sr1 = torchaudio.load(enroll_path)\n",
    "        test_waveform, sr2 = torchaudio.load(test_path)\n",
    "        return label, enroll_waveform, test_waveform, sr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. Pre-trained Speaker Verification Model Wrapper\n",
    "#    (Using Hugging Face’s transformers – https://github.com/huggingface/transformers)\n",
    "# =============================================================================\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "\n",
    "class SpeakerVerificationModel(nn.Module):\n",
    "    def __init__(self, model_name='microsoft/wavlm-base-plus'):\n",
    "        \"\"\"\n",
    "        Loads a pre‑trained model and adds a projection head.\n",
    "        \"\"\"\n",
    "        super(SpeakerVerificationModel, self).__init__()\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = WavLMModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = self.model.config.hidden_size\n",
    "        self.fc = nn.Linear(self.embedding_dim, self.embedding_dim)  # Projection head\n",
    "       \n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        waveforms: can be a list of raw audio signals or a tensor of shape [batch, channels, time].\n",
    "        If waveforms is a tensor with a singleton channel dimension, it is squeezed to shape [batch, time].\n",
    "        \"\"\"\n",
    "        # If input is a tensor with shape [batch, 1, time], squeeze to [batch, time]\n",
    "        if isinstance(waveforms, torch.Tensor):\n",
    "            if waveforms.dim() == 3 and waveforms.size(1) == 1:\n",
    "                waveforms = waveforms.squeeze(1)  # shape now: [batch, time]\n",
    "            # Optionally, if waveforms are batched as [batch, time], convert to list of arrays if needed.\n",
    "            # waveforms = waveforms.tolist() if waveforms.ndim == 2 else waveforms\n",
    "            # Convert each sample to a numpy array\n",
    "            waveforms = [w.cpu().numpy() for w in waveforms]\n",
    "        \n",
    "        inputs = self.feature_extractor(waveforms, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(next(self.parameters()).device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        # Mean pooling over time dimension\n",
    "        hidden_states = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings = self.fc(hidden_states)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. Evaluation Metrics for Speaker Verification\n",
    "# =============================================================================\n",
    "def compute_eer(scores, labels):\n",
    "    \"\"\"\n",
    "    Computes Equal Error Rate (EER) given similarity scores and binary labels.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    return eer * 100\n",
    "\n",
    "def compute_tar_at_far(scores, labels, target_far=0.01):\n",
    "    \"\"\"\n",
    "    Computes True Accept Rate (TAR) at a given False Accept Rate (FAR).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    idx = np.argmin(np.abs(fpr - target_far))\n",
    "    tar = tpr[idx]\n",
    "    return tar * 100\n",
    "\n",
    "def compute_identification_accuracy(labels, scores, threshold=0.5):\n",
    "    predictions = [1 if score >= threshold else 0 for score in scores]\n",
    "    correct = sum(1 for pred, label in zip(predictions, labels) if pred == label)\n",
    "    accuracy = correct / len(labels) * 100  # Convert to percentage\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. Training Function for Speaker Verification Model (Fine-tuning with LoRA + ArcFace)\n",
    "# =============================================================================\n",
    "def train_speaker_verification(model, train_loader, optimizer, arcface_loss, id2label, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        identities, waveforms, sr = batch\n",
    "        # Assume waveforms are padded tensors; if not, additional collate_fn is needed.\n",
    "        waveforms = waveforms.to(device)\n",
    "        # Map identities to integer labels using id2label dict\n",
    "        labels = torch.tensor([id2label[i] for i in identities]).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(waveforms)\n",
    "        loss = arcface_loss(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        del waveforms, embeddings, labels, loss\n",
    "        # Call garbage collection and empty the CUDA cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. Multi-Speaker Dataset Creation (Mixing Utterances)\n",
    "#    (Inspired by LibriMix: https://github.com/JorisCos/LibriMix/blob/master/generate_librimix.sh)\n",
    "# =============================================================================\n",
    "def mix_utterances(file1, file2, target_sr=16000, snr_db=0):\n",
    "    \"\"\"\n",
    "    Loads two audio files and creates a mixture.\n",
    "    Here we simply sum the waveforms (with equal energy).\n",
    "    \"\"\"\n",
    "    y1, sr1 = librosa.load(file1, sr=target_sr)\n",
    "    y2, sr2 = librosa.load(file2, sr=target_sr)\n",
    "    # Truncate to same length\n",
    "    min_len = min(len(y1), len(y2))\n",
    "    y1 = y1[:min_len]\n",
    "    y2 = y2[:min_len]\n",
    "    # (Optional: adjust relative energy based on desired SNR.)\n",
    "    mixture = y1 + y2\n",
    "    mixture = mixture / np.max(np.abs(mixture) + 1e-7)\n",
    "    return mixture, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSpeakerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates a multi-speaker dataset by pairing utterances from two speakers.\n",
    "    For VoxCeleb2, we assume that for each identity there is a metadata txt file\n",
    "    (in vox2/txt) and corresponding audio in vox2/aac.\n",
    "    \"\"\"\n",
    "    def __init__(self, identity_list, metadata_dir, audio_dir, transform=None, max_duration=3.0, target_sr=16000):\n",
    "        self.samples_by_id = {}\n",
    "        self.transform = transform\n",
    "        self.max_duration = max_duration  # in seconds, e.g., 3.0\n",
    "        self.target_sr = target_sr\n",
    "        # Collect all available utterances for each identity\n",
    "        for identity in identity_list:\n",
    "            meta_files = glob.glob(os.path.join(metadata_dir, identity, '*.txt'))\n",
    "            files = []\n",
    "            for meta_file in meta_files:\n",
    "                with open(meta_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        audio_file = line.strip()  # assumed to be relative path\n",
    "                        full_audio_path = os.path.join(audio_dir, identity, audio_file)\n",
    "                        files.append(full_audio_path)\n",
    "            if files:\n",
    "                self.samples_by_id[identity] = files\n",
    "        # Randomly pair utterances for mixing\n",
    "        # random.shuffle(self.samples)\n",
    "        # self.pairs = []\n",
    "        # for i in range(0, len(self.samples) - 1, 2):\n",
    "        #     self.pairs.append((self.samples[i], self.samples[i+1]))\n",
    "        self.identities = list(self.samples_by_id.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return len(self.pairs)\n",
    "        return math.comb(len(self.identities), 2)  # arbitraryly chosen for pairing\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # (id1, file1), (id2, file2) = self.pairs[idx]\n",
    "        # mixture, source1, source2 = mix_utterances(file1, file2)\n",
    "        # return (id1, id2), mixture, source1, source2\n",
    "        #####################################################################\n",
    "        # id1, id2 = random.sample(self.identities, 2)\n",
    "        # file1 = random.choice(self.samples_by_id[id1])\n",
    "        # file2 = random.choice(self.samples_by_id[id2])\n",
    "        # mixture, source1, source2 = mix_utterances(file1, file2)\n",
    "        # return (id1, id2), mixture, source1, source2\n",
    "        ######################################################################\n",
    "        # Randomly choose two different speakers\n",
    "        id1, id2 = random.sample(self.identities, 2)\n",
    "        file1 = random.choice(self.samples_by_id[id1])\n",
    "        file2 = random.choice(self.samples_by_id[id2])\n",
    "        # Create a mixture from the two audio files.\n",
    "        mixture, source1, source2 = mix_utterances(file1, file2, target_sr=self.target_sr)\n",
    "        # If a maximum duration is specified, trim the signals.\n",
    "        if self.max_duration is not None:\n",
    "            max_samples = int(self.max_duration * self.target_sr)\n",
    "            mixture = mixture[:max_samples]\n",
    "            source1 = source1[:max_samples]\n",
    "            source2 = source2[:max_samples]\n",
    "        return (id1, id2), mixture, source1, source2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. Pre-trained SepFormer Wrapper for Speech Separation & Enhancement\n",
    "#    (Using SpeechBrain’s pre-trained SepFormer model: https://huggingface.co/speechbrain/sepformer-whamr)\n",
    "# =============================================================================\n",
    "class SepFormerWrapper:\n",
    "    def __init__(self, model_name=\"speechbrain/sepformer-whamr\"):\n",
    "        from speechbrain.pretrained import SepformerSeparation as SepFormer\n",
    "        self.model = SepFormer.from_hparams(source=model_name, savedir=\"pretrained_sepformer\")\n",
    "    \n",
    "    def separate(self, mixture, sample_rate=8000):\n",
    "        \"\"\"\n",
    "        Writes the mixture to a temporary file, calls the SepFormer separation,\n",
    "        and returns the estimated sources.\n",
    "        \"\"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "            sf.write(temp_file.name, mixture, sample_rate)\n",
    "            temp_filename = temp_file.name\n",
    "        est_sources = self.model.separate_file(temp_filename)\n",
    "        os.remove(temp_filename)\n",
    "        return est_sources  # list of separated sources (as numpy arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. Novel Pipeline Combining Speaker Identification and SepFormer for\n",
    "#    Joint Separation & Enhancement.\n",
    "# =============================================================================\n",
    "class SpeakerSeparationPipeline(nn.Module):\n",
    "    def __init__(self, sepformer_wrapper, speaker_verification_model):\n",
    "        \"\"\"\n",
    "        sepformer_wrapper: an instance of SepFormerWrapper for inference-only enhancement.\n",
    "        speaker_verification_model: the speaker identification model.\n",
    "        \"\"\"\n",
    "        super(SpeakerSeparationPipeline, self).__init__()\n",
    "        # The SepFormer is used in inference mode only (it is frozen and not part of the backpropagation).\n",
    "        self.sepformer = sepformer_wrapper  \n",
    "        self.speaker_model = speaker_verification_model\n",
    "\n",
    "    def forward(self, mixture):\n",
    "        \"\"\"\n",
    "        mixture: Tensor of shape (1, T)\n",
    "        Returns:\n",
    "            enhanced_sources_tensors: list of tensors of enhanced speech signals.\n",
    "            embeddings: list of speaker embeddings computed from each enhanced source.\n",
    "        \"\"\"\n",
    "        # Convert the mixture tensor to a numpy array (SepFormerWrapper expects numpy input).\n",
    "        mixture_np = mixture.squeeze(0).cpu().numpy()\n",
    "        # Call the real pre-trained SepFormer model for speech enhancement.\n",
    "        enhanced_sources = self.sepformer.separate(mixture_np, sample_rate=8000)\n",
    "        enhanced_sources_tensors = []\n",
    "        embeddings = []\n",
    "        for src in enhanced_sources:\n",
    "            # Convert each enhanced source back to a tensor.\n",
    "            src_tensor = torch.tensor(src).float().to(next(self.speaker_model.parameters()).device).unsqueeze(0)\n",
    "            enhanced_sources_tensors.append(src_tensor)\n",
    "            # Compute the speaker embedding using the speaker identification model.\n",
    "            emb = self.speaker_model(src_tensor)\n",
    "            embeddings.append(emb)\n",
    "        return enhanced_sources_tensors, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. Training and Evaluation Functions for the New Pipeline\n",
    "# =============================================================================\n",
    "def train_pipeline(pipeline, train_loader, optimizer, criterion, id2label, device):\n",
    "    pipeline.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        speaker_ids, mixture, source1, source2 = batch\n",
    "        mixture_tensor = torch.tensor(mixture).float().to(device).unsqueeze(0)  # shape (1, T)\n",
    "        optimizer.zero_grad()\n",
    "        separated_sources, embeddings = pipeline(mixture_tensor)\n",
    "        # Create ground truth labels for each separated source using speaker_ids\n",
    "        # Here, we assume a mapping (id2label) exists; both speakers are used.\n",
    "        labels = torch.tensor([id2label[speaker_ids[0]], id2label[speaker_ids[1]]]).to(device)\n",
    "        # Concatenate embeddings (assume each embedding is 1xD)\n",
    "        emb_concat = torch.cat(embeddings, dim=0)\n",
    "        loss = criterion(emb_concat, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        del labels, emb_concat, loss, embeddings, separated_sources\n",
    "        # Call garbage collection and empty the CUDA cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_sepformer_baseline(test_loader, sepformer_wrapper, sv_model, enrollment_db, device):\n",
    "    \"\"\"\n",
    "    Evaluate the multi-speaker test set using the pre-trained SepFormer alone (baseline).\n",
    "    For each mixture, the function:\n",
    "      - Uses SepFormer to separate and enhance the mixture.\n",
    "      - Computes SDR, SIR, SAR (via mir_eval) and PESQ.\n",
    "      - Computes Rank-1 speaker identification accuracy by comparing\n",
    "        each separated source embedding (from sv_model) to the enrollment database.\n",
    "        \n",
    "    Args:\n",
    "        test_loader (DataLoader): DataLoader for the multi-speaker test set.\n",
    "        sepformer_wrapper (SepFormerWrapper): Pre-trained SepFormer for enhancement.\n",
    "        sv_model (nn.Module): Speaker verification model.\n",
    "        enrollment_db (dict): Mapping from speaker_id to enrollment embedding.\n",
    "        device (torch.device): Computation device.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with keys \"SDR\", \"SIR\", \"SAR\", \"PESQ\", \"Rank1_Identification_Accuracy\".\n",
    "    \"\"\"\n",
    "    sv_model.eval()\n",
    "    all_sdr, all_sir, all_sar, all_pesq = [], [], [], []\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        # Each batch: (speaker_ids, mixture, source1, source2)\n",
    "        speaker_ids, mixture, source1, source2 = batch  # speaker_ids is a tuple/list, e.g., (spk1, spk2)\n",
    "        # Convert mixture (assumed as a numpy array or list) to tensor\n",
    "        mixture_tensor = torch.tensor(mixture).float().to(device).unsqueeze(0)  # shape: [1, T]\n",
    "        # Convert mixture to numpy array for SepFormerWrapper (expects numpy input)\n",
    "        mixture_np = mixture_tensor.squeeze(0).cpu().numpy()\n",
    "        # Use pre-trained SepFormer to separate/enhance the mixture\n",
    "        enhanced_sources = sepformer_wrapper.separate(mixture_np, sample_rate=8000)  # list of np arrays\n",
    "        \n",
    "        # Ground truth sources (assumed to be numpy arrays or lists)\n",
    "        gt_sources = [np.array(source1), np.array(source2)]\n",
    "        # Align lengths: truncate signals to the minimum length among estimates and ground truths\n",
    "        min_len = min(len(gt_sources[0]), len(enhanced_sources[0]), len(gt_sources[1]), len(enhanced_sources[1]))\n",
    "        gt_sources = [src[:min_len] for src in gt_sources]\n",
    "        est_sources = [src[:min_len] for src in enhanced_sources]\n",
    "        \n",
    "        # Compute separation metrics using mir_eval\n",
    "        try:\n",
    "            sdr, sir, sar, _ = mir_eval.separation.bss_eval_sources(np.stack(gt_sources), np.stack(est_sources))\n",
    "            all_sdr.append(np.mean(sdr))\n",
    "            all_sir.append(np.mean(sir))\n",
    "            all_sar.append(np.mean(sar))\n",
    "        except Exception as e:\n",
    "            print(\"Error computing mir_eval metrics:\", e)\n",
    "        \n",
    "        # Compute PESQ for each source (using sample_rate=8000 and 'wb' mode)\n",
    "        pesq_scores = []\n",
    "        for i in range(len(gt_sources)):\n",
    "            try:\n",
    "                score = pesq(8000, gt_sources[i], est_sources[i], 'wb')\n",
    "                pesq_scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(\"Error computing PESQ:\", e)\n",
    "        if pesq_scores:\n",
    "            all_pesq.append(np.mean(pesq_scores))\n",
    "        else:\n",
    "            all_pesq.append(0.0)\n",
    "        \n",
    "        # Speaker Identification: For each enhanced source, compute its embedding using sv_model\n",
    "        enhanced_embeddings = []\n",
    "        for est in est_sources:\n",
    "            est_tensor = torch.tensor(est).float().to(device).unsqueeze(0)  # shape: [1, T]\n",
    "            emb = sv_model(est_tensor)\n",
    "            enhanced_embeddings.append(emb)\n",
    "        \n",
    "        # For each computed embedding, perform nearest neighbor search against the enrollment_db\n",
    "        for emb, true_spk in zip(enhanced_embeddings, speaker_ids):\n",
    "            emb = emb.squeeze(0)  # shape: [embedding_dim]\n",
    "            max_sim = -1\n",
    "            pred_spk = None\n",
    "            for spk, enroll_emb in enrollment_db.items():\n",
    "                sim = F.cosine_similarity(emb.unsqueeze(0), enroll_emb.unsqueeze(0)).item()\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    pred_spk = spk\n",
    "            if pred_spk == true_spk:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "    \n",
    "    rank1_accuracy = (correct_count / total_count * 100) if total_count > 0 else 0.0\n",
    "    results = {\n",
    "        \"SDR\": np.mean(all_sdr) if all_sdr else None,\n",
    "        \"SIR\": np.mean(all_sir) if all_sir else None,\n",
    "        \"SAR\": np.mean(all_sar) if all_sar else None,\n",
    "        \"PESQ\": np.mean(all_pesq) if all_pesq else None,\n",
    "        \"Rank1_Identification_Accuracy\": rank1_accuracy\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enrollment_db(enrollment_data, speaker_model, device):\n",
    "    \"\"\"\n",
    "    Computes enrollment embeddings for each speaker.\n",
    "    \n",
    "    Args:\n",
    "        enrollment_data (dict): Mapping from speaker_id (str) to enrollment file path.\n",
    "        speaker_model (nn.Module): Pre-trained/fine-tuned speaker verification model.\n",
    "        device (torch.device): Device to run computations on.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Mapping from speaker_id to enrollment embedding (torch.Tensor).\n",
    "    \"\"\"\n",
    "    enrollment_db = {}\n",
    "    for spk, file_path in enrollment_data.items():\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        waveform = waveform.to(device)\n",
    "        # Ensure waveform shape is [batch, time]\n",
    "        if waveform.dim() == 3 and waveform.size(1) == 1:\n",
    "            waveform = waveform.squeeze(1)\n",
    "        # Compute embedding (unsqueeze for batch dimension)\n",
    "        emb = speaker_model(waveform.unsqueeze(0))\n",
    "        enrollment_db[spk] = emb.squeeze(0)  # store as [embedding_dim]\n",
    "    return enrollment_db\n",
    "\n",
    "def evaluate_pipeline(pipeline, test_loader, enrollment_db, device):\n",
    "    \"\"\"\n",
    "    Evaluate the pipeline on the multi-speaker test dataset.\n",
    "    Computes SDR, SIR, SAR using mir_eval, PESQ using the pesq package,\n",
    "    and actual Rank‑1 identification accuracy by comparing each enhanced source\n",
    "    embedding to an enrollment database.\n",
    "    \n",
    "    Args:\n",
    "        pipeline (nn.Module): The combined speaker separation/enhancement pipeline.\n",
    "        test_loader (DataLoader): DataLoader for the multi-speaker test set.\n",
    "        enrollment_db (dict): Mapping from speaker_id (str) to enrollment embedding.\n",
    "        device (torch.device): Device to run computations on.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics (SDR, SIR, SAR, PESQ, Rank‑1 Identification Accuracy).\n",
    "    \"\"\"\n",
    "    pipeline.eval()\n",
    "    all_sdr, all_sir, all_sar, all_pesq = [], [], [], []\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        # Assume batch returns: (speaker_ids, mixture, source1, source2)\n",
    "        speaker_ids, mixture, source1, source2 = batch  # speaker_ids is a tuple/list, e.g., (spk1, spk2)\n",
    "        # Convert mixture (assumed as numpy array or list) to tensor.\n",
    "        mixture_tensor = torch.tensor(mixture).float().to(device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            enhanced_sources_tensors, embeddings = pipeline(mixture_tensor)\n",
    "        \n",
    "        # Convert enhanced outputs to numpy arrays for mir_eval.\n",
    "        est_sources_np = [src.squeeze(0).detach().cpu().numpy() for src in enhanced_sources_tensors]\n",
    "        # Ground truth sources from dataset (convert to numpy arrays if not already)\n",
    "        gt_sources = [np.array(source1), np.array(source2)]\n",
    "        # Align lengths: truncate all signals to the minimum length.\n",
    "        min_len = min(len(gt_sources[0]), len(est_sources_np[0]), len(gt_sources[1]), len(est_sources_np[1]))\n",
    "        gt_sources = [src[:min_len] for src in gt_sources]\n",
    "        est_sources_np = [src[:min_len] for src in est_sources_np]\n",
    "        \n",
    "        # Compute separation metrics using mir_eval\n",
    "        try:\n",
    "            sdr, sir, sar, _ = mir_eval.separation.bss_eval_sources(np.stack(gt_sources), np.stack(est_sources_np))\n",
    "            all_sdr.append(np.mean(sdr))\n",
    "            all_sir.append(np.mean(sir))\n",
    "            all_sar.append(np.mean(sar))\n",
    "        except Exception as e:\n",
    "            print(\"Error computing mir_eval metrics:\", e)\n",
    "        \n",
    "        # Compute PESQ (sample_rate=8000, mode 'wb' for wideband)\n",
    "        pesq_scores = []\n",
    "        for i in range(len(gt_sources)):\n",
    "            try:\n",
    "                score = pesq(8000, gt_sources[i], est_sources_np[i], 'wb')\n",
    "                pesq_scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(\"Error computing PESQ:\", e)\n",
    "        if pesq_scores:\n",
    "            all_pesq.append(np.mean(pesq_scores))\n",
    "        else:\n",
    "            all_pesq.append(0.0)\n",
    "        \n",
    "        # Compute actual Rank-1 Identification Accuracy:\n",
    "        # For each enhanced source embedding, compare to enrollment_db using cosine similarity.\n",
    "        # Assume that ordering of embeddings corresponds to ordering in speaker_ids.\n",
    "        for emb, true_spk in zip(embeddings, speaker_ids):\n",
    "            emb = emb.squeeze(0)  # shape: [embedding_dim]\n",
    "            max_sim = -1\n",
    "            pred_spk = None\n",
    "            for spk, enroll_emb in enrollment_db.items():\n",
    "                sim = F.cosine_similarity(emb.unsqueeze(0), enroll_emb.unsqueeze(0)).item()\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    pred_spk = spk\n",
    "            if pred_spk == true_spk:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "    rank1_accuracy = (correct_count / total_count * 100) if total_count > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "        \"SDR\": np.mean(all_sdr) if all_sdr else None,\n",
    "        \"SIR\": np.mean(all_sir) if all_sir else None,\n",
    "        \"SAR\": np.mean(all_sar) if all_sar else None,\n",
    "        \"PESQ\": np.mean(all_pesq) if all_pesq else None,\n",
    "        \"Rank1_Identification_Accuracy\": rank1_accuracy\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11. Main Execution Pipeline\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for VoxCelebTrialDataset.\n",
    "    Each item in batch: (label, enroll_waveform, test_waveform, sr)\n",
    "    Pads enrollment and test waveforms to the max length in the batch.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    enroll_waveforms = []\n",
    "    test_waveforms = []\n",
    "    srs = []\n",
    "    \n",
    "    # Determine max lengths for enrollment and test waveforms separately\n",
    "    max_len_enroll = max(item[1].size(1) for item in batch)\n",
    "    max_len_test = max(item[2].size(1) for item in batch)\n",
    "    \n",
    "    for label, enroll_waveform, test_waveform, sr in batch:\n",
    "        labels.append(label)\n",
    "        # Pad enrollment waveform (assumed shape: [channels, time])\n",
    "        if enroll_waveform.size(1) < max_len_enroll:\n",
    "            pad_amt = max_len_enroll - enroll_waveform.size(1)\n",
    "            enroll_waveform = F.pad(enroll_waveform, (0, pad_amt))\n",
    "        enroll_waveforms.append(enroll_waveform)\n",
    "        \n",
    "        # Pad test waveform\n",
    "        if test_waveform.size(1) < max_len_test:\n",
    "            pad_amt = max_len_test - test_waveform.size(1)\n",
    "            test_waveform = F.pad(test_waveform, (0, pad_amt))\n",
    "        test_waveforms.append(test_waveform)\n",
    "        \n",
    "        srs.append(sr)\n",
    "    \n",
    "    # Stack padded tensors and convert labels to tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    enroll_waveforms = torch.stack(enroll_waveforms)\n",
    "    test_waveforms = torch.stack(test_waveforms)\n",
    "    \n",
    "    return labels, enroll_waveforms, test_waveforms, srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# I. Speaker Verification Evaluation (Pre-trained)\n",
    "# ----------------------------\n",
    "sv_model = SpeakerVerificationModel(model_name='microsoft/wavlm-base-plus').to(device)\n",
    "trial_dataset = VoxCelebTrialDataset(trial_file=\"vox1/trial_pairs.txt\", root_dir=\"vox1/wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_loader = DataLoader(trial_dataset, batch_size=4, shuffle=False,collate_fn=trial_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_model_pretrained = copy.deepcopy(sv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "scores, labels_list = [], []\n",
    "sv_model_pretrained.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in trial_loader:  # Limit to first 2 batches for demonstration\n",
    "        label, enroll_waveform, test_waveform, sr = batch\n",
    "        enroll_waveform = enroll_waveform.to(device)\n",
    "        test_waveform = test_waveform.to(device)\n",
    "        enroll_emb = sv_model_pretrained(enroll_waveform)\n",
    "        test_emb = sv_model_pretrained(test_waveform)\n",
    "        cosine_sim = F.cosine_similarity(enroll_emb, test_emb)\n",
    "        scores.extend(cosine_sim.cpu().numpy().tolist())\n",
    "        labels_list.extend(label)\n",
    "        del enroll_waveform, test_waveform, enroll_emb, test_emb, cosine_sim\n",
    "        # Call garbage collection and empty the CUDA cache\n",
    "        gc.collect()\n",
    "        # Free unused memory after each batch\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Speaker Verification: EER = 49.28%, TAR@1%FAR = 2.79%, Speaker Identification Accuracy = 50.48%\n"
     ]
    }
   ],
   "source": [
    "eer = compute_eer(np.array(scores), np.array(labels_list))\n",
    "tar = compute_tar_at_far(np.array(scores), np.array(labels_list), target_far=0.01)\n",
    "compute_identification_accuracy(labels_list, scores, threshold=0.5)\n",
    "# Print results\n",
    "print(f\"Pre-trained Speaker Verification: EER = {eer:.2f}%, TAR@1%FAR = {tar:.2f}%, Speaker Identification Accuracy = {compute_identification_accuracy(labels_list, scores, threshold=0.5):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def voxceleb_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for VoxCelebDataset.\n",
    "    Each item in the batch is (identity, waveform, sr).\n",
    "    Pads each waveform to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    identities = []\n",
    "    waveforms = []\n",
    "    srs = []\n",
    "    \n",
    "    # Determine maximum length across the batch.\n",
    "    max_len = max(item[1].size(1) for item in batch)\n",
    "    \n",
    "    for identity, waveform, sr in batch:\n",
    "        identities.append(identity)\n",
    "        # Pad waveform on the time dimension if needed.\n",
    "        if waveform.size(1) < max_len:\n",
    "            pad_amt = max_len - waveform.size(1)\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "        waveforms.append(waveform)\n",
    "        srs.append(sr)\n",
    "    \n",
    "    waveforms = torch.stack(waveforms, dim=0)\n",
    "    return identities, waveforms, srs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def create_enrollment_data(root_dir):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping speaker IDs to an enrollment audio file path.\n",
    "    \n",
    "    Assumes structure: root_dir/<speaker_id>/<subfolder>/<utterance_file>.wav.\n",
    "    For each speaker, the function recursively searches through subdirectories and selects the first found .wav file.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the directory containing speaker folders.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Mapping from speaker_id (str) to enrollment audio file path (str).\n",
    "    \"\"\"\n",
    "    enrollment_data = {}\n",
    "    # List speaker directories in sorted order\n",
    "    speaker_dirs = sorted(os.listdir(root_dir))\n",
    "    for speaker in speaker_dirs:\n",
    "        speaker_path = os.path.join(root_dir, speaker)\n",
    "        if os.path.isdir(speaker_path):\n",
    "            # Recursively search for .m4a files in all subfolders\n",
    "            m4a_files = glob.glob(os.path.join(speaker_path, '**', '*.m4a'), recursive=True)\n",
    "            if m4a_files:\n",
    "                # Select the first m4a file as the enrollment utterance\n",
    "                enrollment_data[speaker] = m4a_files[0]\n",
    "    return enrollment_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_audio_dir = \"vox2/aac\"\n",
    "enrollment_data = create_enrollment_data(root_audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Enrollment Data:\")\n",
    "# for spk, file_path in enrollment_data.items():\n",
    "#     print(f\"Speaker {spk}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m enrollment_db \u001b[38;5;241m=\u001b[39m compute_enrollment_db(enrollment_data, sv_model_pretrained, device)\n",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m, in \u001b[0;36mcompute_enrollment_db\u001b[1;34m(enrollment_data, speaker_model, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spk, file_path \u001b[38;5;129;01min\u001b[39;00m enrollment_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     15\u001b[0m     waveform, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(file_path)\n\u001b[1;32m---> 16\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Ensure waveform shape is [batch, time]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m waveform\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m waveform\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "enrollment_db = compute_enrollment_db(enrollment_data, sv_model_pretrained, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment_db_pretrained = compute_enrollment_db(enrollment_data, sv_model_pretrained, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Speaker Verification with 42 train identities, testing on 0 identities.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m apply_lora(sv_model, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Instantiate ArcFace loss (using embedding dimension from model)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m arcface_loss \u001b[38;5;241m=\u001b[39m ArcFaceLoss(embedding_size\u001b[38;5;241m=\u001b[39msv_model\u001b[38;5;241m.\u001b[39membedding_dim, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_ids))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(sv_model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(arcface_loss\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     23\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1161\u001b[0m         device,\n\u001b[0;32m   1162\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1163\u001b[0m         non_blocking,\n\u001b[0;32m   1164\u001b[0m     )\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# II. Fine-tuning Speaker Verification with VoxCeleb2 using LoRA and ArcFace\n",
    "# ---------------------------------------------------------------------------\n",
    "identities = sorted(os.listdir(\"vox2/txt\"))\n",
    "train_ids = identities[:100]\n",
    "test_ids = identities[100:]\n",
    "print(f\"Fine-tuning Speaker Verification with {len(train_ids)} train identities, testing on {len(test_ids)} identities.\")\n",
    "# Create a mapping from identity to label index\n",
    "trim_transform = lambda x: trim_audio(x, sr=16000, max_duration=3.0)\n",
    "train_dataset = VoxCelebDataset(root_dir=\"vox2/aac\", identities=train_ids,transform=trim_transform, file_ext='.m4a')\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=voxceleb_collate_fn)\n",
    "\n",
    "# Create a mapping from identity to label index\n",
    "id2label = {identity: idx for idx, identity in enumerate(train_ids)}\n",
    "\n",
    "# Apply LoRA to the speaker verification model\n",
    "apply_lora(sv_model, r=4, alpha=1.0)\n",
    "\n",
    "# Instantiate ArcFace loss (using embedding dimension from model)\n",
    "arcface_loss = ArcFaceLoss(embedding_size=sv_model.embedding_dim, num_classes=len(train_ids)).to(device)\n",
    "optimizer = torch.optim.Adam(list(sv_model.parameters()) + list(arcface_loss.parameters()), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_speaker_verification(sv_model, train_loader, optimizer, arcface_loss, id2label, device)\n",
    "    print(f\"Fine-tuning Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate fine-tuned model on VoxCeleb1 trial pairs\n",
    "scores_ft, labels_list_ft = [], []\n",
    "sv_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in trial_loader:\n",
    "        label, enroll_waveform, test_waveform, sr = batch\n",
    "        enroll_waveform = enroll_waveform.to(device)\n",
    "        test_waveform = test_waveform.to(device)\n",
    "        enroll_emb = sv_model(enroll_waveform)\n",
    "        test_emb = sv_model(test_waveform)\n",
    "        cosine_sim = F.cosine_similarity(enroll_emb, test_emb)\n",
    "        scores_ft.extend(cosine_sim.cpu().numpy().tolist())\n",
    "        labels_list_ft.extend(label)\n",
    "        del enroll_waveform, test_waveform, enroll_emb, test_emb, cosine_sim\n",
    "        # Call garbage collection and empty the CUDA cache\n",
    "        gc.collect()\n",
    "        # Free unused memory after each batch\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer_ft = compute_eer(np.array(scores_ft), np.array(labels_list_ft))\n",
    "tar_ft = compute_tar_at_far(np.array(scores_ft), np.array(labels_list_ft), target_far=0.01)\n",
    "compute_identification_accuracy(labels_list_ft, scores_ft, threshold=0.5)\n",
    "print(f\"Fine-tuned Speaker Verification: EER = {eer_ft:.2f}%, TAR@1%FAR = {tar_ft:.2f}%, Fine Tuned Speaker Identification Accuracy = {compute_identification_accuracy(labels_list, scores, threshold=0.5):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment_db_finetuned = compute_enrollment_db(enrollment_data, sv_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# III. Create Multi-Speaker Scenario Dataset (Mixing VoxCeleb2 Utterances)\n",
    "# ----------------------------\n",
    "identities_multi = sorted(os.listdir(\"vox2/txt\"))\n",
    "train_multi_ids = identities_multi[:50]\n",
    "test_multi_ids = identities_multi[50:100]\n",
    "\n",
    "train_multi_dataset = MultiSpeakerDataset(identity_list=train_multi_ids,\n",
    "                                            metadata_dir=\"vox2/txt\",\n",
    "                                            audio_dir=\"vox2/aac\", max_duration=3.0, target_sr=16000)\n",
    "test_multi_dataset = MultiSpeakerDataset(identity_list=test_multi_ids,\n",
    "                                            metadata_dir=\"vox2/txt\",\n",
    "                                            audio_dir=\"vox2/aac\", max_duration=3.0, target_sr=16000)\n",
    "print(f\"Multi-Speaker Dataset: {len(train_multi_dataset)} training samples, {len(test_multi_dataset)} testing samples.\")\n",
    "train_multi_loader = DataLoader(train_multi_dataset, batch_size=4, shuffle=True)\n",
    "test_multi_loader = DataLoader(test_multi_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# IV. Speech Separation & Enhancement using SepFormer\n",
    "# ----------------------------\n",
    "sepformer_wrapper = SepFormerWrapper(model_name=\"speechbrain/sepformer-whamr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Baseline Evaluation using pre-trained SepFormer\n",
    "# ----------------------------\n",
    "baseline_results = evaluate_sepformer_baseline(test_multi_loader, sepformer_wrapper, sv_model_pretrained, enrollment_db_pretrained, device)\n",
    "print(\"Baseline Evaluation with pre-trained SepFormer and Speaker Verification:\")\n",
    "for metric, value in baseline_results.items():\n",
    "    print(f\"{metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# V. Novel Pipeline: Combining Speaker Identification with SepFormer\n",
    "# ----------------------------\n",
    "pipeline = SpeakerSeparationPipeline(sepformer_wrapper=sepformer_wrapper,speaker_verification_model=sv_model).to(device)\n",
    "pipeline_optimizer = torch.optim.Adam(pipeline.parameters(), lr=1e-4)\n",
    "pipeline_criterion = ArcFaceLoss(embedding_size=sv_model.embedding_dim, num_classes=len(test_multi_ids)).to(device)\n",
    "\n",
    "num_pipeline_epochs = 5\n",
    "# For the pipeline training, we create a dummy id2label mapping for the test multi-speaker set.\n",
    "id2label_multi = {identity: idx for idx, identity in enumerate(test_multi_ids)}\n",
    "for epoch in range(num_pipeline_epochs):\n",
    "    loss = train_pipeline(pipeline, train_multi_loader, pipeline_optimizer, pipeline_criterion, id2label_multi, device)\n",
    "    print(f\"Pipeline Training Epoch {epoch+1}/{num_pipeline_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate the novel pipeline on the multi-speaker test set\n",
    "pipeline_results = evaluate_pipeline(pipeline, test_multi_loader, enrollment_db_finetuned, device)\n",
    "print(\"Final Evaluation Results for Novel Pipeline on Multi-Speaker Test Set:\")\n",
    "for metric, value in pipeline_results.items():\n",
    "    print(f\"{metric}: {value:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
